#!/bin/bash
# ============================================================================
# OpenNovelty Environment Configuration Example
# ============================================================================
# Usage:
#   1. Copy this file to .env: cp .env_example .env
#   2. Fill in your actual API Keys and configurations
#   3. Load it: source .env (or auto-loaded by scripts)
# ============================================================================

# ============================================================================
# Layer 1: Essential Configuration (⭐ Required)
# ============================================================================

# ------------ LLM API Configuration (Required) ------------
# Purpose: Text generation and analysis for Phase 1-4

export LLM_API_ENDPOINT="https://openrouter.ai/api/v1"  # API endpoint (OpenRouter or OpenAI compatible)
export LLM_API_KEY="sk-xxxxxxxxxxxxxxxxxxxxxxxx"        # ← Fill in your API Key
export LLM_MODEL_NAME="anthropic/claude-sonnet-4.5"     # Model name (e.g., anthropic/claude-sonnet-4.5, gpt-4o)

# Examples for different providers:
# - OpenRouter: https://openrouter.ai/api/v1 with model "anthropic/claude-sonnet-4.5"
# - OpenAI:     https://api.openai.com/v1 with model "gpt-4o"
# - Other:      Any OpenAI-compatible endpoint


# ------------ Wispaper API Configuration (Required for Phase 2) ------------
# Purpose: Semantic paper search

# Token file path (default: ~/.wispaper_tokens.json, no config needed)
# To customize path, uncomment and modify the line below:
# export WISPAPER_TOKEN_FILE="/path/to/your/wispaper_tokens.json"

# ⚠️ First-time setup required (run authentication script):
#    python scripts/refresh_wispaper_token.py
#    → Opens browser for Wispaper login (register first: https://wispaper.ai)
#    → Token saved and auto-refreshed for long-term use

# ============================================================================
# Layer 2: Optional Optimization Configuration
# ============================================================================

# ------------ Advanced LLM Configuration (Optional) ------------

# Dedicated model for Taxonomy generation (Phase 3 taxonomy tree)
# If not configured, uses main LLM configuration above
# export TAXONOMY_LLM_API_ENDPOINT="https://openrouter.ai/api/v1"
# export TAXONOMY_LLM_API_KEY="sk-xxxxxxxxxxxxxxxxxxxxxxxx"
# export TAXONOMY_LLM_MODEL_NAME="anthropic/claude-sonnet-4.5"


# LLM Output Token Limits
# export LLM_MAX_TOKENS="8000"              # Max output tokens per request (default: 8000)
# export LLM_PROVIDER_CAP="64000"           # Provider hard cap (default: 64000)
# export LLM_MAX_PROMPT_CHARS="250000"      # Max prompt characters (default: 250000)

# ------------ Network Proxy Configuration (Optional) ------------

# export HTTP_PROXY="http://127.0.0.1:7893"
# export HTTPS_PROXY="http://127.0.0.1:7893"

# ============================================================================
# Layer 3: Advanced Tuning Configuration (Performance & Debugging)
# ============================================================================

# ------------ Phase 2 Search Configuration ------------

# Query concurrency (default: 1, keep 1 for batch processing to avoid API rate limits)
# export PHASE2_QUERY_CONCURRENCY="1"

# Require all queries to succeed (default: true, abort on failure)
# export PHASE2_REQUIRE_100_SUCCESS="true"

# Retry attempts per query (default: 8)
# export PHASE2_MAX_QUERY_ATTEMPTS="8"

# ------------ Phase 3 Analysis Configuration ------------

# Skip textual similarity detection (Phase 3 Part 3)
# ⚠️ Recommended to set true: this method is still under development
export SKIP_TEXTUAL_SIMILARITY="true"

# Notes:
# - true: Skip Part 3 textual similarity detection, other Phase 3 steps continue normally
# - false: Run full textual similarity detection (experimental feature)
# - Skipping this step does not affect final report quality

# ------------ Logging Configuration ------------

# export LOG_LEVEL="INFO"                   # Log level: DEBUG, INFO, WARNING, ERROR
# export LOG_FILE="pipeline.log"            # Log file path (set "none" to disable file logging)
# export LOG_MAX_BYTES="10485760"           # Max log file size (10MB)
# export LOG_BACKUP_COUNT="5"               # Number of log files to keep

# ------------ Retry and Timeout Configuration ------------

# export MAX_RETRIES="30"                   # Max retry attempts (default: 30)
# export RETRY_DELAY="5"                    # Retry delay in seconds (default: 5)
# export PDF_DOWNLOAD_MAX_RETRIES="5"       # Max PDF download retries (default: 5)
# export REQUEST_TIMEOUT="120"              # HTTP request timeout in seconds (default: 120)

# ------------ Other Configuration ------------

# export MAX_SEARCH_RESULTS="10"            # Max search results per query (default: 10)
# export MAX_CONTRIBUTIONS_PER_PAPER="5"    # Max contributions per paper (default: 5)
# export MAX_CONTEXT_CHARS="200000"         # Max LLM context characters (default: 200000)

# ============================================================================
# Configuration Verification
# ============================================================================
# Verify LLM configuration:
#   echo "LLM_PROVIDER: $LLM_PROVIDER"
#   echo "LLM_MODEL_NAME: $LLM_MODEL_NAME"
#   echo "LLM_API_KEY: ${LLM_API_KEY:0:10}..."  # Show first 10 characters only
#
# Verify Wispaper Token:
#   python -c "from paper_novelty_pipeline.services.wispaper_client import WispaperClient; WispaperClient()"
#   # Should see "Loaded token bundle" for success
# ============================================================================
